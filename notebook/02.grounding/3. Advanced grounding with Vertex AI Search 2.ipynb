{"cells":[{"cell_type":"markdown","metadata":{"id":"Vo6hw1koPHTL"},"source":["Copyright 2024 - Forusone : shins777@gmail.com\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","   https://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"oAmrBjzC-weK"},"source":["# Advanced RAG with Vertex AI Search as a grounding service.\n","\n","* This notebook explains how to use grounding service in Gemini Pro.\n","* Refer to https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview\n","* Using Vertex AI Search :\n","  * https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview#ground-private"]},{"cell_type":"markdown","metadata":{"id":"G-cUpjAMUQZ9"},"source":["# Configuration\n","## Install python packages\n","* Vertex AI SDK for Python\n","  * https://cloud.google.com/python/docs/reference/aiplatform/latest\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWV1C2BLrkGR"},"outputs":[],"source":["%pip install --upgrade --quiet google-cloud-aiplatform google-cloud-discoveryengine"]},{"cell_type":"code","source":["from IPython.display import display, Markdown"],"metadata":{"id":"MqSTf34-WHuK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjgasEpKBgMC"},"source":["## Authentication to access to the GCP & Google drive\n","\n","* Use OAuth to access the GCP environment.\n"," * Refer to the authentication methods in GCP : https://cloud.google.com/docs/authentication?hl=ko"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHUPlLRgNBgS"},"outputs":[],"source":["#  For only colab to authenticate to get an access to the GCP.\n","import sys\n","\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()\n","\n","    # To access contents in Google drive\n","    # from google.colab import drive\n","    # drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"DGDQkGBB4_UE"},"source":["## Set the environment on GCP Project"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7ZEsQhjo7Tz"},"outputs":[],"source":["MODEL_NAME=\"gemini-1.5-flash\"\n","PROJECT_ID=\"ai-hangsik\"\n","REGION=\"asia-northeast3\"\n","\n"]},{"cell_type":"markdown","source":["### Vertex AI initialization\n","Configure Vertex AI and access to the foundation model.\n","* Vertex AI initialization : aiplatform.init(..)\n","  * https://cloud.google.com/python/docs/reference/aiplatform/latest#initialization"],"metadata":{"id":"3Dtv_t6GVV87"}},{"cell_type":"code","source":["import os\n","import time\n","import ast\n","import requests\n","import json\n","import numpy as np\n","from concurrent.futures import ThreadPoolExecutor\n","from operator import is_not\n","from functools import partial\n","\n","import google\n","import google.oauth2.credentials\n","import google.auth.transport.requests\n","from google.oauth2 import service_account\n","\n","import vertexai\n","from vertexai.generative_models import (\n","    GenerationConfig,\n","    GenerativeModel,\n","    HarmBlockThreshold,\n","    HarmCategory,\n","    Part,\n",")\n","\n","from google.cloud import discoveryengine_v1alpha as discoveryengine\n","\n","# Initalizate the current vertex AI execution environment.\n","vertexai.init(project=PROJECT_ID, location=REGION)\n"],"metadata":{"id":"ipMgcPZnVYlh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ranking service"],"metadata":{"id":"qxdYYHi-R6Vv"}},{"cell_type":"code","source":["#--------[ Ranking service ]---------------\n","from google.cloud import discoveryengine_v1alpha as discoveryengine\n","\n","re_rank_client = discoveryengine.RankServiceClient()\n","\n","ranking_config = re_rank_client.ranking_config_path(\n","    project=PROJECT_ID,\n","    location=\"global\",\n","    ranking_config=\"default_ranking_config\",\n",")\n","\n","RANKER_MODEL = f\"semantic-ranker-512@latest\"\n","RANK_SCORE = 0.3"],"metadata":{"id":"37qibme7R4w3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Vertex AI Search end point URL"],"metadata":{"id":"2-aJxyzGGrbT"}},{"cell_type":"code","source":["\n","\n","# Constant - Configuration\n","SEARCH_URL = \"https://discoveryengine.googleapis.com/v1alpha/projects/721521243942/locations/global/collections/default_collection/dataStores/it-laws-ds_1713063479348/servingConfigs/default_search:search\"\n","NUM_SEARCH = 2\n","NUM_QUESTION = 3\n","\n","# Credential token for REST call to Vertex AI\n","stream = os.popen('gcloud auth print-access-token')\n","CREDENTIAL_TOKEN = stream.read().strip()\n","\n"],"metadata":{"id":"PW0VW6h-GvdI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Process for RAG\n","\n"," 1. Classify complex questions into several.\n"," 2. Search each question through Vertex AI Search.\n"," 3. Only the necessary context is organized through the verification process of the search results.\n"," 4. Configure only the verified context as the final context and request the final result from LLM.\n"],"metadata":{"id":"CUTNRa5SmBrV"}},{"cell_type":"code","source":["\n","def rag_process(question:str, condition:dict, top_n: int = 5 ):\n","    \"\"\"\n","    Controller to execute the RAG processes.\n","    \"\"\"\n","\n","    print(f\"[rag_process] start rag_process : {question}\")\n","\n","    t1 = time.perf_counter()\n","\n","    rewrited_questions = rewrite_question(question)\n","    rewrited_questions.append(question) # Add original question\n","    print(f\"[rag_process] rewrited_questions : {rewrited_questions}\")\n","\n","    t2 = time.perf_counter()\n","\n","    searched_list = search_contexts(rewrited_questions)\n","    print(f\"[rag_process] searched_list : {searched_list}\")\n","\n","    t3 = time.perf_counter()\n","\n","    ranked_results = ranking_results(question, searched_list, top_n)\n","    print(f\"[rag_process] ranked_results : {ranked_results}\")\n","\n","    t4 = time.perf_counter()\n","\n","    final_contexts = \"\\n Searched Context : \".join(ranked_results)\n","\n","    final_prompt = f\"\"\"\n","      You are an AI assistant that searches for knowledge and provides some advice.\n","      When answering the <Question> below, please refer only to the contents within <Context>, infer step by step, summarize, and answer.\n","\n","      <Context>{final_contexts}</Context>\n","      <Question>{question}</Question>\n","    \"\"\"\n","\n","    final_outcome = gemini_response(final_prompt)\n","    t5 = time.perf_counter()\n","\n","    execution_stat['query_rewrite'] = round((t2-t1), 3)\n","    execution_stat['vertex_ai_search'] = round((t3-t2), 3)\n","    execution_stat['ranked_results'] = round((t4-t3), 3)\n","    execution_stat['llm_request'] = round((t5-t4), 3)\n","\n","    return final_outcome, execution_stat\n","\n","#----------------------------------------------------------------------------------------------------------------\n","\n","def rewrite_question(question:str )->list:\n","\n","  prompt = f\"\"\"\n","    This is a question generator for your precise search.\n","    In order to search for facts to answer the [Question] below, please create 3 questions based on [Question].\n","    Your answers must be in the list format below.\n","\n","      [Question] : {question}\n","      Format : [\"Question 1\", \"Question 2\", \"Question 3\"]\n","\n","  \"\"\"\n","  questions = gemini_response(prompt)\n","  print(f\"[Controller][rewrite_question] questions : {questions}\")\n","\n","  q_list = []\n","\n","  try:\n","      q_list = ast.literal_eval(questions)\n","\n","  # Handling for exception when splitting mixed question.\n","  except Exception as e:\n","      print(f\"[Controller][rewrite_question] Query rewrite failed\")\n","      for i in range(NUM_QUESTION):\n","          q_list.append(question)\n","\n","  print(f\"[Controller][rewrite_question] Generated Question List : {q_list}\")\n","\n","  return q_list\n","\n","\n","#----------------------------------------------------------------------------------------------------------------\n","\n","def search_contexts(rewrited_questions):\n","    \"\"\"\n","    Controller to execute the RAG processes.\n","\n","    1. Call flow for mixed question:\n","        question_splitter --> search_chunks\n","    2. Call flow for singuar question:\n","        search_chunks\n","    \"\"\"\n","\n","    # t1 = time.perf_counter()\n","\n","    # Parallel processing to reduce the latency for the Vertex AI Search.\n","    with ThreadPoolExecutor(max_workers=10) as executor:\n","        searched_contexts = executor.map(search_parsing, rewrited_questions )\n","\n","    searched_list = [context for context in searched_contexts]\n","\n","    print(f\"[Controller][search] len(searched_list) : {len(searched_list)}\")\n","    print(f\"[Controller][search] searched_list : {searched_list}\")\n","\n","    return searched_list\n","\n","\n","\n","#----------------------------------------------------------------------------------------------\n","\n","def search_parsing(question:str)->str:\n","    print(f\"[Controller][search_parsing] search_parsing Start! : {question}\")\n","\n","    #------- Searching --------------------------------------------------------------------\n","    # request = google.auth.transport.requests.Request()\n","    # Controller.credentials.refresh(request)\n","\n","    headers = {\n","        \"Authorization\": \"Bearer \"+ CREDENTIAL_TOKEN,\n","        \"Content-Type\": \"application/json\"\n","    }\n","\n","    query_dic ={\n","        \"query\": question,\n","        \"page_size\": str(NUM_SEARCH),\n","        \"offset\": 0,\n","        \"contentSearchSpec\":{\n","                \"searchResultMode\" : \"CHUNKS\",\n","                \"chunkSpec\" : {\n","                    \"numPreviousChunks\" : 1,\n","                    \"numNextChunks\" : 1\n","                }\n","        },\n","    }\n","\n","    data = json.dumps(query_dic)\n","    data=data.encode(\"utf8\")\n","    response = requests.post(SEARCH_URL,headers=headers, data=data)\n","\n","    print(f\"[Controller][search_parsing] Search Response len : {len(response.text)}\")\n","    #print(f\"[Controller][search_parsing] Search Response chunks : {response.text}\")\n","    print(f\"[Controller][search_parsing] Search End! : {question}\")\n","\n","    # Start to parse the searched chunks\n","    dict_results = json.loads(response.text)\n","\n","    #------- Parsing --------------------------------------------------------------------\n","\n","    search_results = {}\n","\n","    if dict_results.get('results'):\n","        for result in dict_results['results']:\n","            item = {}\n","            chunk = result['chunk']\n","            item['title'] = chunk['documentMetadata']['title']\n","            item['uri'] = chunk['documentMetadata']['uri']\n","            item['pageSpan'] = f\"{chunk['pageSpan']['pageStart']} ~ {chunk['pageSpan']['pageEnd']}\"\n","            item['content'] = chunk['content']\n","            item['question'] = question\n","\n","            if 'chunkMetadata' in chunk:\n","                add_chunks = chunk['chunkMetadata']\n","                if 'previousChunks' in add_chunks:\n","                    # Chunks appearing from those closest to the current Contents.\n","                    p_chunks = chunk['chunkMetadata']['previousChunks']\n","                    if p_chunks:\n","                        for p_chunk in p_chunks:\n","                            item['content'] = p_chunk['content'] +\"\\n\"+ item['content']\n","\n","                if 'nextChunks' in add_chunks:\n","                    n_chunks = chunk['chunkMetadata']['nextChunks']\n","                    if n_chunks:\n","                        for n_chunk in n_chunks:\n","                            item['content'] = item['content'] +\"\\n\"+ n_chunk['content']\n","\n","            search_results['result'] = item\n","\n","    return search_results\n","\n","#----------------------------------------------------------------------------------------------------------------\n","def ranking_results(query, search_results, top_n):\n","\n","    records = []\n","\n","    for index, response in enumerate(search_results):\n","\n","      title = response['result']['title']\n","      content = response['result']['content']\n","      records.append(discoveryengine.RankingRecord(id=str(index), title=title, content=content))\n","\n","    request = discoveryengine.RankRequest(\n","        ranking_config = ranking_config,\n","        model = RANKER_MODEL,\n","        top_n = top_n,\n","        query = query,\n","        records = records\n","      )\n","\n","    ranked_response = re_rank_client.rank(request=request,)\n","\n","    ranked_res_list = []\n","\n","    for record in ranked_response.records:  # https://cloud.google.com/generative-ai-app-builder/docs/reference/rpc/google.cloud.discoveryengine.v1alpha#rankresponse\n","\n","      if record.score > RANK_SCORE:\n","        print(f\"ranking score > {RANK_SCORE} : {record.score}\")\n","        print(f\"Ranked result : [{record.score}] : {record.content}\")\n","        ranked_res_list.append(record.content)\n","      else:\n","        print(f\"ranking score < {RANK_SCORE} : {record.score}\")\n","    return ranked_res_list\n","\n","\n","#----------------------------------------------------------------------------------------------------------------\n","\n","def gemini_response(prompt:str,\n","                response_schema:dict = None):\n","\n","    model = GenerativeModel(model_name=MODEL_NAME)\n","\n","    generation_config = GenerationConfig(\n","        temperature=0.5,\n","        top_p=1.0,\n","        top_k=32,\n","        candidate_count=1,\n","        max_output_tokens=8192,\n","        )\n","\n","    responses = model.generate_content(\n","        [prompt],\n","        generation_config = generation_config)\n","\n","    print(f\"[Controller][call_gemini] Final response Len {len(responses.text)}\")\n","\n","    return responses.text"],"metadata":{"id":"t4YFV6zrfSOn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Execute RAG"],"metadata":{"id":"tlju3lgmlJ0K"}},{"cell_type":"code","source":["\n","from time import perf_counter\n","\n","t1 = perf_counter()\n","\n","question = \"개인정보 분쟁조정위원회는 어떤 조직이고 역할은 무엇인가요?\"\n","final_outcome, execution_stat = rag_process(question, 5)\n","\n","display(Markdown(final_outcome))\n","\n","t2  = perf_counter()\n","\n","print(f\"\\n Total latency : {t2 - t1} seconds\\n\\n\")\n","print(f\"\\n Detailed latency : {execution_stat} \\n\\n\")\n"],"metadata":{"id":"p7rYEx-mWVA9"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1ysth26WNEZ89ceoUWyiBNEhJhAKPzZzD","timestamp":1707791310414},{"file_id":"1SMBsO1Oo1C_iwdVlaPxa-nHarsr1LlXw","timestamp":1707790104224},{"file_id":"12QOQwx4nCfn-TVBJz482kmzE9AO7LOEa","timestamp":1706083956041},{"file_id":"1SEMv5rLuJzrFmgv6ijM-a6frQK6vJh-F","timestamp":1685622927692},{"file_id":"17KvSE1Jozr-l8MPgV0qc96RqjDGib6EG","timestamp":1683740067510},{"file_id":"/v2/external/notebooks/snippets/bigquery.ipynb","timestamp":1674946081821}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"nbformat":4,"nbformat_minor":0}