{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"source":["# Copyright 2024 Forusone(shins777@gmail.com)\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"JAPoU8Sm5E6e"},"source":["# Context Caching\n","* [Gemini Context Caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview)\n","  * Use context caching to reduce the cost of requests that contain repeat content with high input token counts. Cached context items, such as a large amount of text, an audio file, or a video file, can be used in prompt requests to the Gemini API to generate output.\n","  * For example, each prompt request that composes a chat conversation might include the same context cache that references a video along with unique text that comprises each turn in the chat. The minimum size of a context cache is 32,768 tokens.\n","\n","* [Supported MIME types](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview#supported-mime-types)\n"]},{"cell_type":"markdown","metadata":{"id":"61RBz8LLbxCR"},"source":["## Set configuration"]},{"cell_type":"markdown","metadata":{"id":"No17Cw5hgx12"},"source":["### Install packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tFy3H3aPgx12","executionInfo":{"status":"ok","timestamp":1735168726060,"user_tz":-540,"elapsed":6730,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["%pip install --upgrade --user --quiet google-cloud-aiplatform"]},{"cell_type":"markdown","metadata":{"id":"dmWOrTJ3gx13"},"source":["### Authentication to access to GCP\n","* Only for Colab in Google Drive\n","* No need to do this process if in Colab Enteprise on Vertex AI."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"NyKGtVQjgx13","executionInfo":{"status":"ok","timestamp":1735168729474,"user_tz":-540,"elapsed":1064,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# To use markdown for output data from LLM\n","from IPython.display import display, Markdown\n","\n","# Use OAuth to access the GCP environment.\n","import sys\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"]},{"cell_type":"markdown","source":["## Lab Execution"],"metadata":{"id":"COGJ9ZvT3mrX"}},{"cell_type":"markdown","metadata":{"id":"DF4l8DTdWgPY"},"source":["### Define constants"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Nqwi-5ufWp_B","cellView":"form","executionInfo":{"status":"ok","timestamp":1735168732385,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["\n","PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n","LOCATION = \"us-central1\"  # @param {type:\"string\"}\n","MODEL_NAME = \"gemini-1.5-flash-002\" # @param {type:\"string\"}"]},{"cell_type":"markdown","metadata":{"id":"vHwJCyNF6u0O"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mginH0QC6u0O","executionInfo":{"status":"ok","timestamp":1735168742564,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["import vertexai\n","\n","from vertexai.generative_models import (\n","    GenerationConfig,\n","    # GenerativeModel, replace by prview version.\n","    HarmBlockThreshold,\n","    HarmCategory,\n","    GenerationResponse,\n","    Tool,\n","    Part,\n","    ChatSession\n",")\n","\n","from vertexai.preview.generative_models import (\n","    grounding,\n","    GenerativeModel\n",")\n","from vertexai.preview import (\n","    caching\n",")"]},{"cell_type":"markdown","source":["### Initialize Vertex AI"],"metadata":{"id":"hqYHkPad4Ait"}},{"cell_type":"code","source":["\n","# https://cloud.google.com/python/docs/reference/aiplatform/latest#initialization\n","vertexai.init(project=PROJECT_ID, location=LOCATION)\n","\n","# https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.GenerativeModel\n","model = GenerativeModel(MODEL_NAME)"],"metadata":{"id":"ZhCD2bEj4DbP","executionInfo":{"status":"ok","timestamp":1735168747710,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Helper functions"],"metadata":{"id":"EGh-Z-nm4YDc"}},{"cell_type":"markdown","source":["### Considerations for preview version\n","* [Context cache limit](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-create#context_cache_limits)"],"metadata":{"id":"zaW3ZB9D91Tt"}},{"cell_type":"code","source":["#--------------------------------------------------------------------------------------------\n","def create_cache_pdf(pdfs:list, display_name:str):\n","  \"\"\"\n","  Create pdf cache contents\n","\n","  Args:\n","    pdfs: list\n","      pdf file list\n","    display_name: str\n","      display name of the cache content.\n","\n","  Returns:\n","    cached_content: CachedContent\n","      cached content object\n","\n","  \"\"\"\n","\n","  import datetime\n","\n","  system_instruction = \"\"\"\n","    당신은 체계적인 문헌 조사와 다양한 주제에 대한 메타 분석을 수행한 수년간의 경험을 가진 전문 연구자입니다.\n","    당신은 항상 제공된 출처의 사실을 고수하고 새로운 사실을 결코 만들어내지 않습니다.\n","\n","  \"\"\"\n","\n","  contents = [Part.from_uri(pdf, mime_type=\"application/pdf\") for pdf in pdfs ]\n","\n","  cached_content = caching.CachedContent.create(\n","      model_name=MODEL_NAME,\n","      display_name=display_name,\n","      system_instruction=system_instruction,\n","      contents=contents,\n","      ttl=datetime.timedelta(minutes=60),\n","  )\n","\n","  return cached_content\n","\n","#--------------------------------------------------------------------------------------------\n","def generate_with_cache(query:str,\n","                        cache_content_name:str)-> GenerationResponse:\n","\n","  \"\"\"\n","  Generate a response from the model.\n","\n","  Args:\n","    query: str\n","      query to send to the model.\n","    cache_content_name: str\n","      cached content name.\n","\n","  Returns:\n","    resonse: GenerationResponse\n","      generated response.\n","\n","  \"\"\"\n","\n","  cached_content = caching.CachedContent(cached_content_name=cache_content_name)\n","  model = GenerativeModel.from_cached_content(cached_content=cached_content)\n","  response = model.generate_content(query)\n","\n","  return response\n","\n","#--------------------------------------------------------------------------------------------\n","def multi_turn_with_cache(cha:ChatSession,\n","                          query:str, )->GenerationResponse:\n","  \"\"\"\n","  Generate a response from the chat model.\n","\n","  Args:\n","    chat:ChatSession\n","      chat session object.\n","    query: str\n","      query to send to the model.\n","\n","  Returns:\n","    resonse: GenerationResponse\n","      generated response.\n","\n","  \"\"\"\n","  response = chat.send_message(query)\n","  return response\n"],"metadata":{"id":"FZ2mSown4hLb","executionInfo":{"status":"ok","timestamp":1735168932640,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Create context cache"],"metadata":{"id":"99gisoXY-X_G"}},{"cell_type":"code","source":["\n","pdfs = [\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n","        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\"\n","        ]\n","display_name = \"Gemini-pdf-cache\"\n","\n","cached_content = create_cache_pdf(pdfs, display_name)\n","\n","print(cached_content.name)\n","print(cached_content.display_name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbVcWCQ77QpQ","executionInfo":{"status":"ok","timestamp":1735168956136,"user_tz":-540,"elapsed":19223,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"00fe4989-2a94-453b-cd5e-5f3d84f93147"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["7771568087446323200\n","Gemini-pdf-cache\n"]}]},{"cell_type":"markdown","source":["### Use context caching in single turn operation"],"metadata":{"id":"rr-hrw7n-tYv"}},{"cell_type":"code","source":["\n","query = \"이 연구 논문들은 무슨 내용을 이야기하고 있나요?\"\n","response = generate_with_cache(query, cached_content.name )\n","\n","print(response.text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r4rTSBkF7pLv","executionInfo":{"status":"ok","timestamp":1735168972417,"user_tz":-540,"elapsed":13857,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"11e81678-8b02-4d34-9cec-1c326084c494"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["이 연구 논문은 구글 딥마인드의 새로운 다중 모드 모델인 제미니(Gemini) 1.5 Pro에 관한 내용을 다룹니다. 제미니 1.5 Pro는 이미지, 오디오, 비디오 및 텍스트를 이해하는 데 탁월한 능력을 보이는 모델입니다. 이 논문은 제미니 1.5 Pro 모델의 아키텍처, 훈련 인프라, 데이터 세트, 성능 평가 결과 및 책임 있는 배포에 대한 접근 방식을 자세히 설명합니다. 또한, 제미니 1.5 Pro 모델의 특징인 긴 문맥을 처리하는 능력과 다양한 다중 모드 작업에서의 성능을 조명하고, 긴 문맥의 이해 능력에 대한 제한 사항과 이에 대한 후속 연구의 필요성을 지적합니다. 특히, 새로운 언어를 학습하여 번역하는 기능과 같은 놀라운 능력을 보이는 사례를 제시합니다.\n"]}]},{"cell_type":"markdown","metadata":{"id":"tX7vHiybEWeJ"},"source":["### Use context caching in multi-turn chat"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"FYNZS5o0FoGR","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1735169095327,"user_tz":-540,"elapsed":33613,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"bc040934-f9bf-4398-9143-7452a682de5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["사용자: 안녕하세요. \n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"AI Agent : 안녕하세요! 무엇을 도와드릴까요?  Generative AI 기술에 대해 궁금하신 점이 있으시면 언제든지 질문해주세요.  제가 아는 한도 내에서 최선을 다해 설명해 드리겠습니다.  어떤 주제에 대해 알고 싶으신가요? (예: 이미지 생성, 텍스트 생성, 코드 생성 등)\n"},"metadata":{}},{"name":"stdout","output_type":"stream","text":["------------------------------------ \n","사용자: 제미나이가 이미지 생성을 어떻게 하나요 ?\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"AI Agent : 구글의 Gemini는 방대한 이미지와 텍스트 데이터셋을 학습하여 이미지를 생성합니다.  텍스트 설명(프롬프트)을 입력하면, 학습된 정보를 바탕으로 이미지의 구성요소, 스타일, 색상 등을 예측하고 생성합니다.  이는 복잡한 수학적 모델(일반적으로는 확산 모델의 변형)을 사용하여 이루어지며,  픽셀 단위로 이미지를 점진적으로 생성해 나갑니다.  결과적으로 사용자의 프롬프트에 부합하는 이미지가 만들어지는 것입니다.  단순히 텍스트를 이미지로 변환하는 것이 아니라,  학습된 지식을 토대로 새로운 이미지를 생성하는 능력을 갖춘 것이죠.  즉,  새로운 창작물을 만들어내는 것입니다.\n"},"metadata":{}},{"name":"stdout","output_type":"stream","text":["------------------------------------ \n","사용자: 종료\n"]}],"source":["chat = model.start_chat()\n","\n","while True:\n","  query = input('사용자: ')\n","\n","  if query == '종료': break\n","\n","  prompt = f\"\"\"\n","  당신은 Generatie AI 기술을 설명해주는 AI Assistant 입니다.\n","  아래 질문에 대해서 친절하게 10줄 이내로 답해주세요.\n","\n","  질문 : {query}\n","  \"\"\"\n","\n","  response = multi_turn_with_cache(chat, prompt)\n","  display(Markdown(f\"AI Agent : {response.text}\"))\n","  print(f\"------------------------------------ \")\n"]},{"cell_type":"markdown","source":["## Cache context management"],"metadata":{"id":"_80vGz7eE1yA"}},{"cell_type":"markdown","source":["### Management helper functions"],"metadata":{"id":"5u1IGMHoE6OJ"}},{"cell_type":"code","source":["#--------------------------------------------------------------------------------------------\n","def print_cache_content_list():\n","\n","  \"\"\"\n","  Print cache content list in the current project and region that are specified when initializing vertex ai.\n","\n","  Args: None\n","  Returns: None\n","  \"\"\"\n","\n","  cache_list = caching.CachedContent.list()\n","  for cached_content in cache_list:\n","\n","    print_cache_content(cached_content.name)\n","\n","#--------------------------------------------------------------------------------------------\n","def print_cache_content(cache_content_name:str):\n","\n","  \"\"\"\n","  Print a cache content corresponding to the given cache content name.\n","\n","  Args:\n","    cache_content_name: str\n","      cache content name.\n","\n","  Returns: None\n","  \"\"\"\n","\n","  cached_content = caching.CachedContent(cached_content_name=cache_content_name)\n","\n","  print(\"-\"*30)\n","  print(f\"[cache_content]name : {cached_content.display_name}\")\n","  print(f\"[cache_content]name : {cached_content.name}\")\n","  print(f\"[cache_content]resource_name : {cached_content.resource_name}\")\n","  print(f\"[cache_content]model_name : {cached_content.model_name}\")\n","  print(f\"[cache_content]create_time : {cached_content.create_time}\")\n","  print(f\"[cache_content]update_time : {cached_content.update_time}\")\n","  print(f\"[cache_content]expire_time : {cached_content.expire_time}\")\n","\n","#--------------------------------------------------------------------------------------------\n","def update_cache_ttl(cache_content_name:str):\n","\n","  \"\"\"\n","  Update TTL of the cache content corresponding to the given cache content name.\n","\n","  Args:\n","    cache_content_name: str\n","      cache content name.\n","\n","  Returns: None\n","  \"\"\"\n","\n","  import datetime\n","\n","  cached_content = caching.CachedContent(cached_content_name=cache_content_name)\n","  cached_content.update(ttl=datetime.timedelta(minutes=30))\n","\n","  cached_content.refresh()\n","  print(f\"Updated TTL: {cached_content.expire_time}\")\n","\n","  print_cache_content(cached_content.name)\n","\n","#--------------------------------------------------------------------------------------------\n","def delete_cache(cache_content_name:str):\n","  \"\"\"\n","  Delete a cache content corresponding to the given cache content name.\n","\n","  Args:\n","    cache_content_name: str\n","      cache content name.\n","\n","  Returns: None\n","  \"\"\"\n","\n","  cached_content = caching.CachedContent(cached_content_name=cache_content_name)\n","  cached_content.delete()\n","  print(f\"Deleted {cached_content.name}\")\n"],"metadata":{"id":"TU3mevtlE-h3","executionInfo":{"status":"ok","timestamp":1735169143754,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["### Get Cache contents"],"metadata":{"id":"F_O2AkEeD66K"}},{"cell_type":"code","source":["print_cache_content_list()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAaEcg4zDxsG","executionInfo":{"status":"ok","timestamp":1735169180627,"user_tz":-540,"elapsed":2093,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"9b87e476-32e5-4b0d-d8a6-fd261e51ebfb"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------\n","[cache_content]name : Gemini-pdf-cache\n","[cache_content]name : 7771568087446323200\n","[cache_content]resource_name : projects/ai-hangsik/locations/us-central1/cachedContents/7771568087446323200\n","[cache_content]model_name : projects/ai-hangsik/locations/us-central1/publishers/google/models/gemini-1.5-flash-002\n","[cache_content]create_time : 2024-12-25 23:22:17.655480+00:00\n","[cache_content]update_time : 2024-12-25 23:22:17.655480+00:00\n","[cache_content]expire_time : 2024-12-26 00:22:17.645143+00:00\n","------------------------------\n","[cache_content]name : Gemini-pdf-cache\n","[cache_content]name : 6535329989733122048\n","[cache_content]resource_name : projects/ai-hangsik/locations/us-central1/cachedContents/6535329989733122048\n","[cache_content]model_name : projects/ai-hangsik/locations/us-central1/publishers/google/models/gemini-1.5-flash-002\n","[cache_content]create_time : 2024-12-25 23:21:26.172392+00:00\n","[cache_content]update_time : 2024-12-25 23:21:26.172392+00:00\n","[cache_content]expire_time : 2024-12-26 00:21:26.152139+00:00\n"]}]},{"cell_type":"markdown","source":["### Update cache TTL"],"metadata":{"id":"k_RhtOE_Bqeb"}},{"cell_type":"code","source":["cached_content_name = \"151477517935443968\"\n","\n","update_cache_ttl(cached_content_name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2lWzEp8tBmoK","executionInfo":{"status":"ok","timestamp":1735169167376,"user_tz":-540,"elapsed":2310,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"0bf65d37-2816-422c-f6d2-97352f3da1fd"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated TTL: 2024-12-25 23:56:06.179969+00:00\n","------------------------------\n","[cache_content]name : Gemini-pdf-cache\n","[cache_content]name : 151477517935443968\n","[cache_content]resource_name : projects/ai-hangsik/locations/us-central1/cachedContents/151477517935443968\n","[cache_content]model_name : projects/ai-hangsik/locations/us-central1/publishers/google/models/gemini-1.5-flash-002\n","[cache_content]create_time : 2024-12-25 23:19:18.625584+00:00\n","[cache_content]update_time : 2024-12-25 23:26:06.181863+00:00\n","[cache_content]expire_time : 2024-12-25 23:56:06.179969+00:00\n"]}]},{"cell_type":"markdown","source":["### Delete cache"],"metadata":{"id":"T02sU1hNBtyq"}},{"cell_type":"code","source":["cached_content_name = \"151477517935443968\"\n","\n","delete_cache(cached_content_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3tN4YbiBv4d","executionInfo":{"status":"ok","timestamp":1735169175583,"user_tz":-540,"elapsed":1064,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"6abcc187-fd69-47ed-9a17-fd2f7dd775ba"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.base:Deleting CachedContent : projects/ai-hangsik/locations/us-central1/cachedContents/151477517935443968\n"]},{"output_type":"stream","name":"stdout","text":["Deleted 151477517935443968\n"]}]}],"metadata":{"colab":{"toc_visible":true,"provenance":[{"file_id":"1dvDRTW4gcwUa5YiRNkyOrJXG-tthL-ZK","timestamp":1735115360259},{"file_id":"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb","timestamp":1722953478228}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}