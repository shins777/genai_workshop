{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Forusone(shins777@gmail.com)\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB93wBDYDCPI"
   },
   "source": [
    "# Gemini - Chat operation\n",
    "* This notebook shows you how to send chat prompts to the Gemini\n",
    "* Refer to https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vuxu3iRDDtcl"
   },
   "source": [
    "## Set environment\n",
    "### Package install\n",
    "* [google-cloud-aiplatform](https://cloud.google.com/python/docs/reference/aiplatform/latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21491,
     "status": "ok",
     "timestamp": 1731645154046,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "wevZD-jnD-ft",
    "outputId": "c1afbba6-ee34-4d90-9104-ab552f260465",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCVttGUsEzgj"
   },
   "source": [
    "## Authentication to access to GCP\n",
    "\n",
    "* Only Colab in Google Drive\n",
    "* No need to do this process if in Colab Enteprise on Vertex AI.\n",
    "* Refer to the authentication methods in GCP : https://cloud.google.com/docs/authentication?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13593,
     "status": "ok",
     "timestamp": 1731645167638,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "AJuo1g4bE3-x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To use markdown for output data from LLM\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Use OAuth to access the GCP environment.\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lab Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey_Bv55hIblt"
   },
   "source": [
    "### Define constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1731645167638,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "XJwhurOUHIG-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME=\"gemini-1.5-flash\"\n",
    "PROJECT_ID=\"ai-hangsik\"\n",
    "REGION=\"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    ChatSession,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    GenerationResponse,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk3nZQQhIiom"
   },
   "source": [
    "### Initialize Vertex AI\n",
    "* [aiplatform.init](https://cloud.google.com/python/docs/reference/aiplatform/latest#initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://cloud.google.com/python/docs/reference/aiplatform/latest#initialization\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "\n",
    "system_instructions = [\n",
    " \"ë‹¹ì‹ ì€ AI ìŠ¤í˜ì…œë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\",\n",
    " \"ì§ˆë¬¸ ë‚´ìš©ì— ìƒì„¸í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”. ê°€ê¸‰ì  í‘œë¥¼ í†µí•´ì„œ ê°„ëµí•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.\"\n",
    "]\n",
    "\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.GenerativeModel\n",
    "model = GenerativeModel(MODEL_NAME,\n",
    "                        system_instruction=system_instructions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4NbkJ4HJYBB"
   },
   "source": [
    "### Function to get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1731645176424,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "sMaCwPVGJXX6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def interactive_chat(chat: ChatSession, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the model.\n",
    "\n",
    "    - chat : ChatSession Chat session object\n",
    "    - prompt : str - The prompt to send to the model.\n",
    "    - returns: str - The generated response.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set model parameter : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts#set_model_parameters\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 0.95,\n",
    "    }\n",
    "\n",
    "    # Configure satey setting : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes\n",
    "    # Refer to the link to remove : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#how_to_remove_automated_response_blocking_for_select_safety_attributes\n",
    "    safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    }\n",
    "\n",
    "    text_response = []\n",
    "\n",
    "    # https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.language_models.ChatSession\n",
    "    responses = chat.send_message(\n",
    "        question,\n",
    "        generation_config = generation_config,\n",
    "        safety_settings = safety_settings,\n",
    "        tools = None,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return responses.text\n",
    "\n",
    "def get_chat_history(chat):\n",
    "\n",
    "  history = \" \".join([content.text for content in chat.history])\n",
    "  print(f\"---------------- Chat history ---------------- \")\n",
    "  print(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGdT-2WHJuXf"
   },
   "source": [
    "## AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "executionInfo": {
     "elapsed": 124285,
     "status": "ok",
     "timestamp": 1731645344160,
     "user": {
      "displayName": "Hangsik Shin",
      "userId": "04632555686962088332"
     },
     "user_tz": -540
    },
    "id": "ln_Jadw9w0Ew",
    "outputId": "73257cb9-3cb4-4977-abab-4fb4179910b6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì:  ì•ˆë…•í•˜ì„¸ìš”.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "AI Agent : ì•ˆë…•í•˜ì„¸ìš”! ì‚¼ì„±ì „ì ê°€ì „ ì œí’ˆ êµ¬ë§¤ë¥¼ ë„ì™€ë“œë¦´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? \n",
       "\n",
       "ì–´ë–¤ ì œí’ˆì— ê´€ì‹¬ ìˆìœ¼ì‹ ê°€ìš”? í˜¹ì‹œ ì›í•˜ëŠ” ê¸°ëŠ¥ì´ë‚˜ ì˜ˆì‚°ì´ ìˆìœ¼ì‹ ê°€ìš”? \n",
       "\n",
       "ì‚¼ì„±ì „ìëŠ” ë‹¤ì–‘í•œ í˜ì‹ ì ì¸ ê°€ì „ ì œí’ˆë“¤ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ëƒ‰ì¥ê³ , ì„¸íƒê¸°, TV, ì—ì–´ì»¨ ë“± ë‹¤ì–‘í•œ ì œí’ˆ ë¼ì¸ì—…ì„ ê°–ì¶”ê³  ìˆìœ¼ë‹ˆ, ì›í•˜ì‹œëŠ” ì œí’ˆì„ ë§ì”€í•´ì£¼ì„¸ìš”. ìµœê³ ì˜ ì œí’ˆì„ ì°¾ì•„ë“œë¦¬ë„ë¡ ìµœì„ ì„ ë‹¤í•˜ê² ìŠµë‹ˆë‹¤! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------ \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì:  ê³¼ì²œì—ì„œ ëƒ‰ì¥ê³ ëŠ” ì–´ë””ì„œ ì‚¬ì•¼ í•˜ë‚˜ìš” ?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "AI Agent : ê³¼ì²œì—ì„œ ì‚¼ì„± ëƒ‰ì¥ê³ ë¥¼ êµ¬ë§¤í•˜ì‹œë ¤ë©´ ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆìœ¼ì„¸ìš”! ğŸ˜Š\n",
       "\n",
       "* **ì‚¼ì„± ë””ì§€í„¸í”„ë¼ì:** ê³¼ì²œì—ëŠ” ì‚¼ì„± ë””ì§€í„¸í”„ë¼ìê°€ ì§ì ‘ ìš´ì˜í•˜ëŠ” ë§¤ì¥ì´ ì—†ì§€ë§Œ, ê°€ê¹Œìš´ ì•ˆì–‘, ë¶„ë‹¹, ìˆ˜ì› ë“±ì— ìœ„ì¹˜í•œ ë§¤ì¥ì—ì„œ ë‹¤ì–‘í•œ ëƒ‰ì¥ê³  ëª¨ë¸ì„ ì§ì ‘ ë³´ê³  êµ¬ë§¤í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
       "* **ì˜¨ë¼ì¸ ì‡¼í•‘ëª°:** ì‚¼ì„±ë‹·ì»´, ì¿ íŒ¡, Gë§ˆì¼“, 11ë²ˆê°€ ë“± ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì—ì„œë„ ì‚¼ì„± ëƒ‰ì¥ê³ ë¥¼ í¸ë¦¬í•˜ê²Œ êµ¬ë§¤í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¹ ë¥¸ ë°°ì†¡ê³¼ ë‹¤ì–‘í•œ í• ì¸ í˜œíƒì„ ëˆ„ë¦´ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. \n",
       "* **ë°±í™”ì :** ë¡¯ë°ë°±í™”ì , í˜„ëŒ€ë°±í™”ì  ë“± ê³¼ì²œì— ìœ„ì¹˜í•œ ë°±í™”ì ì—ì„œë„ ì‚¼ì„± ëƒ‰ì¥ê³ ë¥¼ íŒë§¤í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë°±í™”ì ì—ì„œëŠ” ì „ë¬¸ ìƒë‹´ê³¼ í•¨ê»˜ ë‹¤ì–‘í•œ ë¸Œëœë“œë¥¼ ë¹„êµí•˜ë©° êµ¬ë§¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "ì–´ë–¤ ë°©ë²•ì´ ê°€ì¥ í¸ë¦¬í•˜ì‹ ê°€ìš”? í˜¹ì‹œ ì›í•˜ì‹œëŠ” ëƒ‰ì¥ê³  ì¢…ë¥˜ë‚˜ ê¸°ëŠ¥ì´ ìˆìœ¼ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------ \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì:  ì¢…ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# @title AI Agent\n",
    "\n",
    "chat = model.start_chat()\n",
    "\n",
    "while True:\n",
    "  query = input('ì‚¬ìš©ì: ')\n",
    "\n",
    "  if query == 'ì¢…ë£Œ': break\n",
    "\n",
    "  # chat_history = get_chat_history(chat)\n",
    "\n",
    "  prompt = f\"\"\"\n",
    "  ë‹¹ì‹ ì€ ì‚¼ì„±ì „ì ê°€ì „ ì œí’ˆì˜ êµ¬ë§¤ë¥¼ ë„ì™€ ì£¼ëŠ” AI Assistant ì…ë‹ˆë‹¤.\n",
    "  ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì¹œì ˆí•˜ê²Œ 10ì¤„ ì´ë‚´ë¡œ ë‹µí•´ì£¼ì„¸ìš”.\n",
    "  ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì‚¼ì„±ì „ì ì œí’ˆì„ ì‚´ìˆ˜ ìˆë„ë¡ ë„ì™€ ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "  ì§ˆë¬¸ : {query}\n",
    "  \"\"\"\n",
    "\n",
    "  response = interactive_chat(chat, prompt)\n",
    "  display(Markdown(f\"AI Agent : {response}\"))\n",
    "  print(f\"------------------------------------ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7ai7JnEDn6H+X+Je3DFHi",
   "provenance": [
    {
     "file_id": "10QiVBSVRSu69Uhr9OpzX3-8CFvMdG4EZ",
     "timestamp": 1717906152865
    },
    {
     "file_id": "1btZCldJxz-DFxshEjVFBksH3cFZ7ZWzY",
     "timestamp": 1717886695950
    }
   ],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
